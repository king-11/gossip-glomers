# Totally Avaible Key Value Store

We are required to build a totally available key value store. The checks that maelstrom does for this are:
- Nodes are always available, `net-timeout` on operation count as being unavailable.
- Follows [Read Uncommitted](#read-uncommitted) consistency model

## Messages

- Transaction `txn`
  - The transaction message consists of multiple operations (read/write) that we need to perform.
  - These are generated by clients.
  - The read operations are returned back with value of the key if present with the node.
- WriteRequest `write`
  - The write message request consists of the latest write values for a key and associated timestamp when it was done.
  - These are generated by nodes and used for replication purposes at a frequency of **1 sec**.
  - We use them in a fire and forget manner as there is no convergence requirement.

## Implementation

The totally available key-value store uses the following architecture:

### Data Structures
- **keyLocks** (`sync.Map`): Per-key RWMutex locks for concurrent access control
- **kv** (`sync.Map`): The actual key-value storage mapping integer keys to integer values
- **lastWrite** (`sync.Map`): Tracks the timestamp of the last write for each key to ensure monotonic writes during replication
- **requestChannel**: Buffered channel to queue write operations for asynchronous replication

### Transaction Processing (`Transaction` method)
1. **Write Lock Acquisition**: Collects all keys that will be written in the transaction, sorts them (to prevent deadlocks), and acquires write locks in order
2. **Operation Execution**: Processes each operation sequentially:
   - **Write operations**: Updates the key-value store and queues the write for replication
   - **Read operations**: Returns the current value or nil if key doesn't exist
3. **Lock Release**: Releases all acquired locks after transaction completion

### Replication Strategy (`WriteServer` goroutine)
- Runs continuously, collecting write operations from the request channel
- Every 1 second, broadcasts accumulated writes to all other nodes using a "write" message
- Fire-and-forget approach - no acknowledgments required (totally available)
- Batches up to 25 writes before sending to reduce network overhead

### Write Convergence (`Write` method)
When receiving replicated writes from other nodes:
1. Acquires locks for all keys being written (ordered to prevent deadlocks)
2. For each write, compares timestamps - only applies writes that are newer than the last known write
3. This ensures eventual convergence while preventing older writes from overwriting newer ones

### Key Design Decisions
- **Ordered Locking**: Keys are always locked in sorted order to prevent deadlocks when multiple transactions access overlapping keys
- **Timestamp-based Conflict Resolution**: Uses timestamps to determine write ordering during replication (last-write-wins)
- **Asynchronous Replication**: Writes are replicated in the background without blocking client operations
- **No Convergence Requirements**: System prioritizes availability over consistency, following read uncommitted semantics

## Read Uncommitted

Read Uncommitted is an incredibly weak consistency model. It prohibits only a single anomaly:
>G0 (dirty write): a cycle of transactions linked by write-write dependencies. For instance, transaction T1 appends 1 to key x, transaction T2 appends 2 to x, and T1 appends 3 to x again, producing the value [1, 2, 3].

Note that read uncommitted does not impose any real-time constraints. If process A completes write w, then process B begins a read r, r is not necessarily guaranteed to observe w.

Moreover, read uncommitted does not require a per-process order between transactions. A process can observe a write, then fail to observe that same write in a subsequent transaction. In fact, a process can fail to observe its own prior writes, if those writes occurred in different transactions.

For instance, a read uncommmitted database can always return the empty state for any reads, by appearing to execute those reads at time 0. It can also discard write-only transactions by reordering them to execute at the very end of the history, after any reads. Operations like increments can also be discarded, assuming the result of the increment is never observed. Luckily, most implementations donâ€™t seem to take advantage of these optimization opportunities.

>[!note] Malestrom can't check for dirty writes.

## Learning

I gave a lot of thought for how replication for read uncommitted should work to provide a totally available behaviour. First thoughts about use of two phase locks won't work in case of a partition.

Then I kept pondering over the idea that dirty writes can happen across nodes when there is a network partition. Finally, I realised that if I prevent Dirty Writes at node level and then during replicatin also ensure to prevent dirty writes at each node then we are safe.

The replication strategy I used was always take the latest value written by any client because it is simple and it makes sense in a lot of senarios. This is taken from the gossip protocol strategy which works really well given we don't have strong convergence requirements.

A critical thing where I encountered a lot of `net-timeout` causing the validation to fail was because of deadlocks. Given, I try to get all the keys that read write lock at once. Using ordered locking helps here and also ensuring not to ask for the same lock twice for multiple operation concerning the same key.
